{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Machine Learning \u00b6","title":"Home"},{"location":"#machine-learning","text":"","title":"Machine Learning"},{"location":"transfer_learning/","text":"Transfer Learning \u00b6 Introduction \u00b6 Sebastian Ruder: Transfer Learning - Machine Learning's Next Frontier","title":"Transfer Learning"},{"location":"transfer_learning/#transfer-learning","text":"","title":"Transfer Learning"},{"location":"transfer_learning/#introduction","text":"Sebastian Ruder: Transfer Learning - Machine Learning's Next Frontier","title":"Introduction"},{"location":"dataset/","text":"Datasets \u00b6 ImageNet \u00b6 download pytorch: imagenet_data = torchvision . datasets . ImageNet ( 'path/to/imagenet_root/' ) data_loader = torch . utils . data . DataLoader ( imagenet_data , batch_size = 4 , shuffle = True , num_workers = args . nThreads )","title":"Datasets"},{"location":"dataset/#datasets","text":"","title":"Datasets"},{"location":"dataset/#imagenet","text":"download pytorch: imagenet_data = torchvision . datasets . ImageNet ( 'path/to/imagenet_root/' ) data_loader = torch . utils . data . DataLoader ( imagenet_data , batch_size = 4 , shuffle = True , num_workers = args . nThreads )","title":"ImageNet"},{"location":"diagram/reinforcement/","text":"Reinforcement Learning \u00b6 Introduction \u00b6 S.Sutton, Richard, and Andrew G.Barto. 2018. Reinforcement Learning: An Introduction. Second Edi. The MIT Press.","title":"Reinforcement Learning"},{"location":"diagram/reinforcement/#reinforcement-learning","text":"","title":"Reinforcement Learning"},{"location":"diagram/reinforcement/#introduction","text":"S.Sutton, Richard, and Andrew G.Barto. 2018. Reinforcement Learning: An Introduction. Second Edi. The MIT Press.","title":"Introduction"},{"location":"diagram/supervised/","text":"Supervised Learning \u00b6 Decision Tree \u00b6 A Decision Tree algorithm is one of the most popular machine learning algorithms. It uses a tree like structure and their possible combinations to solve a particular problem. It belongs to the class of supervised learning algorithms where it can be used for both classification and regression purposes. Tutorial on Karggle Karggle Download notebook Download data for this tutorial Quick References \u00b6 Entropy \u00b6 $$ \\text{Entropy} = \\sum_{i=1}^c - p_i \\log_2(p_i) $$ where \\(c\\) is the number of classes and \\(p_i\\) is the probability associated with the ith class. Gini index \u00b6 $$ \\text{Gini} = 1- \\sum_{i=1}^c p_i^2 $$ where \\(c\\) is the number of classes and \\(p_i\\) is the probability associated with the ith class. Python \u00b6 Category Encoders pip install category_encoders or conda install -c conda-forge category_encoders To use: import category_encoders as ce encoder = ce . BackwardDifferenceEncoder ( cols = [ ... ]) encoder = ce . BaseNEncoder ( cols = [ ... ]) encoder = ce . BinaryEncoder ( cols = [ ... ]) encoder = ce . CatBoostEncoder ( cols = [ ... ]) encoder = ce . CountEncoder ( cols = [ ... ]) encoder = ce . GLMMEncoder ( cols = [ ... ]) encoder = ce . HashingEncoder ( cols = [ ... ]) encoder = ce . HelmertEncoder ( cols = [ ... ]) encoder = ce . JamesSteinEncoder ( cols = [ ... ]) encoder = ce . LeaveOneOutEncoder ( cols = [ ... ]) encoder = ce . MEstimateEncoder ( cols = [ ... ]) encoder = ce . OneHotEncoder ( cols = [ ... ]) encoder = ce . OrdinalEncoder ( cols = [ ... ]) encoder = ce . SumEncoder ( cols = [ ... ]) encoder = ce . PolynomialEncoder ( cols = [ ... ]) encoder = ce . TargetEncoder ( cols = [ ... ]) encoder = ce . WOEEncoder ( cols = [ ... ]) encoder = ce . QuantileEncoder ( cols = [ ... ]) encoder . fit ( X , y ) X_cleaned = encoder . transform ( X_dirty ) https://contrib.scikit-learn.org/category_encoders/ sklearn.tree.DecisionTreeClassifier scikit-learn: sklearn.tree.DecisionTreeClassifier","title":"Supervised Learning"},{"location":"diagram/supervised/#supervised-learning","text":"","title":"Supervised Learning"},{"location":"diagram/supervised/#decision-tree","text":"A Decision Tree algorithm is one of the most popular machine learning algorithms. It uses a tree like structure and their possible combinations to solve a particular problem. It belongs to the class of supervised learning algorithms where it can be used for both classification and regression purposes. Tutorial on Karggle Karggle Download notebook Download data for this tutorial","title":"Decision Tree"},{"location":"diagram/supervised/#quick-references","text":"","title":"Quick References"},{"location":"diagram/supervised/#entropy","text":"$$ \\text{Entropy} = \\sum_{i=1}^c - p_i \\log_2(p_i) $$ where \\(c\\) is the number of classes and \\(p_i\\) is the probability associated with the ith class.","title":"Entropy"},{"location":"diagram/supervised/#gini-index","text":"$$ \\text{Gini} = 1- \\sum_{i=1}^c p_i^2 $$ where \\(c\\) is the number of classes and \\(p_i\\) is the probability associated with the ith class.","title":"Gini index"},{"location":"diagram/supervised/#python","text":"Category Encoders pip install category_encoders or conda install -c conda-forge category_encoders To use: import category_encoders as ce encoder = ce . BackwardDifferenceEncoder ( cols = [ ... ]) encoder = ce . BaseNEncoder ( cols = [ ... ]) encoder = ce . BinaryEncoder ( cols = [ ... ]) encoder = ce . CatBoostEncoder ( cols = [ ... ]) encoder = ce . CountEncoder ( cols = [ ... ]) encoder = ce . GLMMEncoder ( cols = [ ... ]) encoder = ce . HashingEncoder ( cols = [ ... ]) encoder = ce . HelmertEncoder ( cols = [ ... ]) encoder = ce . JamesSteinEncoder ( cols = [ ... ]) encoder = ce . LeaveOneOutEncoder ( cols = [ ... ]) encoder = ce . MEstimateEncoder ( cols = [ ... ]) encoder = ce . OneHotEncoder ( cols = [ ... ]) encoder = ce . OrdinalEncoder ( cols = [ ... ]) encoder = ce . SumEncoder ( cols = [ ... ]) encoder = ce . PolynomialEncoder ( cols = [ ... ]) encoder = ce . TargetEncoder ( cols = [ ... ]) encoder = ce . WOEEncoder ( cols = [ ... ]) encoder = ce . QuantileEncoder ( cols = [ ... ]) encoder . fit ( X , y ) X_cleaned = encoder . transform ( X_dirty ) https://contrib.scikit-learn.org/category_encoders/ sklearn.tree.DecisionTreeClassifier scikit-learn: sklearn.tree.DecisionTreeClassifier","title":"Python"},{"location":"diagram/unsupervised/","text":"","title":"Unsupervised Learning"},{"location":"model/","text":"Model \u00b6 Deterministic Model \u00b6 Discriminative models draw boundaries in the data space, while generative models try to model how data is placed throughout the space. A generative model focuses on explaining how the data was generated, while a discriminative model focuses on predicting the labels of the data. In the case of discriminative models, to find the probability, they directly assume some functional form for \\(P(Y|X)\\) and then estimate the parameters of \\(P(Y|X)\\) with the help of the training data. Generative Model \u00b6 In the case of generative models, to find the conditional probability \\(P(Y|X)\\) , they estimate the prior probability \\(P(Y)\\) and likelihood probability \\(P(X|Y)\\) with the help of the training data and uses the Bayes Theorem to calculate the posterior probability \\(P(Y |X)\\) . \\[ P(Y|X) = \\text{posterior} = \\frac{prior \\times likelihood}{evidence} = \\frac{P(Y) P(X|Y)}{P(X)} \\] Autoregressive \u00b6 An AR(1) autoregressive process is one in which the current value is based on the immediately preceding value, while an AR(2) process is one in which the current value is based on the previous two values. An AR(0) process is used for white noise and has no dependence between the terms. Autoregressive By JASON FERNANDO A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock's future prices based on its past performance. Autoregressive models predict future values based on past values. They are widely used in technical analysis to forecast future security prices. Autoregressive models implicitly assume that the future will resemble the past. Therefore, they can prove inaccurate under certain market conditions, such as financial crises or periods of rapid technological change. original text Get Models \u00b6 Pytorch Models \u00b6 Pytorch Models & Pre-trained Weights ModelZoo \u00b6 ModelZoo Pytorch Image Models (timm) \u00b6 GitHub Repository official doc fastai's doc","title":"Model"},{"location":"model/#model","text":"","title":"Model"},{"location":"model/#deterministic-model","text":"Discriminative models draw boundaries in the data space, while generative models try to model how data is placed throughout the space. A generative model focuses on explaining how the data was generated, while a discriminative model focuses on predicting the labels of the data. In the case of discriminative models, to find the probability, they directly assume some functional form for \\(P(Y|X)\\) and then estimate the parameters of \\(P(Y|X)\\) with the help of the training data.","title":"Deterministic Model"},{"location":"model/#generative-model","text":"In the case of generative models, to find the conditional probability \\(P(Y|X)\\) , they estimate the prior probability \\(P(Y)\\) and likelihood probability \\(P(X|Y)\\) with the help of the training data and uses the Bayes Theorem to calculate the posterior probability \\(P(Y |X)\\) . \\[ P(Y|X) = \\text{posterior} = \\frac{prior \\times likelihood}{evidence} = \\frac{P(Y) P(X|Y)}{P(X)} \\]","title":"Generative Model"},{"location":"model/#autoregressive","text":"An AR(1) autoregressive process is one in which the current value is based on the immediately preceding value, while an AR(2) process is one in which the current value is based on the previous two values. An AR(0) process is used for white noise and has no dependence between the terms. Autoregressive By JASON FERNANDO A statistical model is autoregressive if it predicts future values based on past values. For example, an autoregressive model might seek to predict a stock's future prices based on its past performance. Autoregressive models predict future values based on past values. They are widely used in technical analysis to forecast future security prices. Autoregressive models implicitly assume that the future will resemble the past. Therefore, they can prove inaccurate under certain market conditions, such as financial crises or periods of rapid technological change. original text","title":"Autoregressive"},{"location":"model/#get-models","text":"","title":"Get Models"},{"location":"model/#pytorch-models","text":"Pytorch Models & Pre-trained Weights","title":"Pytorch Models"},{"location":"model/#modelzoo","text":"ModelZoo","title":"ModelZoo"},{"location":"model/#pytorch-image-models-timm","text":"GitHub Repository official doc fastai's doc","title":"Pytorch Image Models (timm)"},{"location":"model/alexnet/","text":"AlexNet \u00b6 Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. \u201cImageNet Classification with Deep Convolutional Neural Networks\u201d In NeurIPS https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html Implementation \u00b6 aaron-xichen/pytorch-playground : Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)","title":"AlexNet"},{"location":"model/alexnet/#alexnet","text":"Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton. 2012. \u201cImageNet Classification with Deep Convolutional Neural Networks\u201d In NeurIPS https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html","title":"AlexNet"},{"location":"model/alexnet/#implementation","text":"aaron-xichen/pytorch-playground : Base pretrained models and datasets in pytorch (MNIST, SVHN, CIFAR10, CIFAR100, STL10, AlexNet, VGG16, VGG19, ResNet, Inception, SqueezeNet)","title":"Implementation"},{"location":"model/inception/","text":"Inception \u00b6 Bharath Raj's explaination of Inception network https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202 Inception v1 \u00b6 Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network. This was popularly known as GoogLeNet (Inception v1). The architecture is shown right. GoogLeNet has 9 such inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last inception module. Paper Szegedy, Christian et al. 2015. \u201cGoing Deeper with Convolutions.\u201d Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 07-12-June-2015: 1\u20139. Inception v2 & v3 \u00b6 Paper Szegedy, Christian et al. 2016. \u201cRethinking the Inception Architecture for Computer Vision.\u201d Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-December: 2818\u201326. Quick References \u00b6 representational bottleneck \u00b6 Reduce representational bottleneck. The intuition was that, neural networks perform better when convolutions didn\u2019t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a \u201crepresentational bottleneck\u201d","title":"Inception"},{"location":"model/inception/#inception","text":"Bharath Raj's explaination of Inception network https://towardsdatascience.com/a-simple-guide-to-the-versions-of-the-inception-network-7fc52b863202","title":"Inception"},{"location":"model/inception/#inception-v1","text":"Very deep networks are prone to overfitting. It also hard to pass gradient updates through the entire network. This was popularly known as GoogLeNet (Inception v1). The architecture is shown right. GoogLeNet has 9 such inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last inception module. Paper Szegedy, Christian et al. 2015. \u201cGoing Deeper with Convolutions.\u201d Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 07-12-June-2015: 1\u20139.","title":"Inception v1"},{"location":"model/inception/#inception-v2-v3","text":"Paper Szegedy, Christian et al. 2016. \u201cRethinking the Inception Architecture for Computer Vision.\u201d Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition 2016-December: 2818\u201326.","title":"Inception v2 &amp; v3"},{"location":"model/inception/#quick-references","text":"","title":"Quick References"},{"location":"model/inception/#representational-bottleneck","text":"Reduce representational bottleneck. The intuition was that, neural networks perform better when convolutions didn\u2019t alter the dimensions of the input drastically. Reducing the dimensions too much may cause loss of information, known as a \u201crepresentational bottleneck\u201d","title":"representational bottleneck"},{"location":"model/transformer/","text":"Transformer \u00b6 Vaswani, Ashish et al. 2017. \u201cAttention Is All You Need.\u201d Advances in Neural Information Processing Systems 2017-Decem(Nips): 5999\u20136009. http://arxiv.org/abs/1706.03762 . The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers are built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention. Like recurrent neural networks (RNNs), transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, transformers do not necessarily process the data in order. Rather, the attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not need to process the beginning of the sentence before the end. Rather it identifies the context that confers meaning to each word in the sentence. This feature allows for more parallelization than RNNs and therefore reduces training times. Training \u00b6 Optimizer \u00b6 \\[ \\text{lrate} = d_{model}^{-0.5} \\cdot \\min (\\text{step_num}^{-0.5}, \\text{step_num}\\cdot\\text{warmup_steps}^{-1.5}) \\] where \\(\\text{warmup_step}\\) used in paper is 4000, \\(d_{model} = 512\\) . plot Mathematica Module [{ dModel = 512 , warmupSteps = 4000 }, Plot [ dModel ^ -0.5 Min [ stepNum ^ -0.5 , stepNum warmupSteps ^ -1.5 ], { stepNum , 0 , 10 ^ 5 }, PlotRange -> All , PlotTheme -> \"Detailed\" , FrameLabel -> { \"step_num\" , \"lrate\" }] ] Implementation \u00b6 leviswind/pytorch-transformer","title":"Transformer"},{"location":"model/transformer/#transformer","text":"Vaswani, Ashish et al. 2017. \u201cAttention Is All You Need.\u201d Advances in Neural Information Processing Systems 2017-Decem(Nips): 5999\u20136009. http://arxiv.org/abs/1706.03762 . The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data. Before transformers, most state-of-the-art NLP systems relied on gated RNNs, such as LSTM and gated recurrent units (GRUs), with added attention mechanisms. Transformers are built on these attention technologies without using an RNN structure, highlighting the fact that attention mechanisms alone can match the performance of RNNs with attention. Like recurrent neural networks (RNNs), transformers are designed to handle sequential input data, such as natural language, for tasks such as translation and text summarization. However, unlike RNNs, transformers do not necessarily process the data in order. Rather, the attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not need to process the beginning of the sentence before the end. Rather it identifies the context that confers meaning to each word in the sentence. This feature allows for more parallelization than RNNs and therefore reduces training times.","title":"Transformer"},{"location":"model/transformer/#training","text":"","title":"Training"},{"location":"model/transformer/#optimizer","text":"\\[ \\text{lrate} = d_{model}^{-0.5} \\cdot \\min (\\text{step_num}^{-0.5}, \\text{step_num}\\cdot\\text{warmup_steps}^{-1.5}) \\] where \\(\\text{warmup_step}\\) used in paper is 4000, \\(d_{model} = 512\\) . plot Mathematica Module [{ dModel = 512 , warmupSteps = 4000 }, Plot [ dModel ^ -0.5 Min [ stepNum ^ -0.5 , stepNum warmupSteps ^ -1.5 ], { stepNum , 0 , 10 ^ 5 }, PlotRange -> All , PlotTheme -> \"Detailed\" , FrameLabel -> { \"step_num\" , \"lrate\" }] ]","title":"Optimizer"},{"location":"model/transformer/#implementation","text":"leviswind/pytorch-transformer","title":"Implementation"}]}